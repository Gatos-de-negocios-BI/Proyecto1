{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score as AS\n",
    "from sklearn.metrics import f1_score as F1\n",
    "from sklearn.metrics import precision_score as PS\n",
    "from sklearn.metrics import recall_score as RS\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data/SuicidiosProyecto.csv\"\n",
    "df = pd.read_csv(ruta, encoding=\"utf-8\", index_col=0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(\".\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"·\", \" \")\n",
    "df['text'] = df['text'].str.replace(\",\", \" \")\n",
    "df['text'] = df['text'].str.replace(\";\", \" \")\n",
    "df['text'] = df['text'].str.replace(\":\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"?\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"¿\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"!\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"¡\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"(\", \" \")\n",
    "df['text'] = df['text'].str.replace(\")\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"[\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"]\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"{\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"}\", \" \")\n",
    "df['text'] = df['text'].str.replace(\"´\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['class'], test_size=0.3, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
    "ax[0].pie(y_train.value_counts(), labels=y_train.value_counts().index, autopct='%1.1f%%')\n",
    "ax[0].set_title('Train')\n",
    "ax[1].pie(y_test.value_counts(), labels=y_test.value_counts().index, autopct='%1.1f%%')\n",
    "ax[1].set_title('Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.union({'4661', 'meeeeeeeeeeee', 'ja', '01457654035', 'reaally', '3624', '3904512441', 'mesooo', 'ان فرع', ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = nltk.TweetTokenizer()\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(tokenizer=tokenize, stop_words=stopwords)\n",
    "x_train_bow = bow.fit_transform(x_train)\n",
    "x_test_bow = bow.transform(x_test)\n",
    "len(bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stopwords)\n",
    "#x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "#x_test_tfidf = tfidf.transform(x_test)\n",
    "#len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random forest usando BoW</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = RandomForestClassifier(random_state = 2, n_estimators= 12, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model.fit(x_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La 'feature importance' en modelos basados en árboles de decisión indican cuales son las características más importantes al momento de tomar una decisión\n",
    "pd.Series(bow_model.feature_importances_, index = bow.vocabulary_).sort_values().tail(40).plot.barh(figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_estimators = bow_model.estimators_\n",
    "print('Number of trees:', len(bow_estimators))\n",
    "print('Trees depth (mean):', np.mean([tree.get_depth() for tree in bow_estimators]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_predict = bow_model.predict(x_train_bow)\n",
    "y_test_bow_predict = bow_model.predict(bow.transform(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(bow_model.classes_) == 2:\n",
    "    print('Precision:', precision_score(y_train, y_train_bow_predict, pos_label = 'suicide'))\n",
    "    print('Recall:', recall_score(y_train, y_train_bow_predict, pos_label = 'suicide'))\n",
    "    print('F1:', f1_score(y_train, y_train_bow_predict, pos_label = 'suicide'))\n",
    "else:\n",
    "    # Para casos no binarios, es común calcular las métricas para cada clase\n",
    "    print('Precision:', precision_score(y_train, y_train_bow_predict, average = None))\n",
    "    print('Recall:', recall_score(y_train, y_train_bow_predict, average = None))\n",
    "    print('F1:', f1_score(y_train, y_train_bow_predict, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(bow_model.classes_) == 2:\n",
    "    print('Precision:', precision_score(y_test, y_test_bow_predict, pos_label = 'suicide'))\n",
    "    print('Recall:', recall_score(y_test, y_test_bow_predict, pos_label = 'suicide'))\n",
    "    print('F1:', f1_score(y_test, y_test_bow_predict, pos_label = 'suicide'))\n",
    "else:\n",
    "    # Para casos no binarios, es común calcular las métricas para cada clase\n",
    "    print('Precision:', precision_score(y_test, y_test_bow_predict, average = None))\n",
    "    print('Recall:', recall_score(y_test, y_test_bow_predict, average = None))\n",
    "    print('F1:', f1_score(y_test, y_test_bow_predict, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date = now.date()\n",
    "time = now.time().strftime(\"%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre=\"modelo_dia_\"+str(date)+\"_hora_\"+str(time)+\".pkl\"\n",
    "pickle.dump(bow_model,open(nombre,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=open(\"primermodelo.pkl\",'rb')\n",
    "#arbolito = pickle.load(f)\n",
    "#print(arbolito)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "680909a85cd0f96ba1c50ae49c7276dc4874e63b5fd1d1624a0b59680dc2cd74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
